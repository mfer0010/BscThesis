\chapter{Background and Literature Review}


\section{The Instruments Used to Gather Data}
%Intro
\paragraph{ }Throughout this study, data recorded as the beam leaves the \acs{SPS} and enters the \acs{LHC} will be used as input parameters to the chosen anomaly detection algorithms. This data was recorded using different censors located in different parts of the injection life cycle. This section describes the different types of censors that were used to collect the data, highlighting the particular details which need to be considered when analysing this data.

%BLMs & TDI BLMs
\paragraph{ }The \ac{BLM} are some of the most safety critical modules of the \acs{LHC} because a loss of a very small fraction of this beam may damage parts of the machine or cause a quench in the superconducting magnets \cite{Holzer2006}. A high beam loss reading could also indicate over-injection. In fact, an injection of a high intensity beam into the LHC is only allowed if there is a low intensity bunch circulating the LHC in order to avoid settings errors \cite{Kain2010}. The \acs{BLM} module is the mostly used module in the current IQC software checks \cite{Drosdal2011}. The \acs{BLM}s must be reliable; the probability of not detecting a dangerous loss was found to be $5\times10^{-6}$ per channel and they are only expected to generate 20 false dumps per year \cite{Holzer2006}. The \acs{BLM}s are extensively logged to a database for offline analysis \cite{Holzer2006}. 

\paragraph{ }For this particular study, the readings logged for the \ac{TDI} \acs{BLM}s and the \ac{TL} \acs{BLM}s in TI2 and TI8 will be used (refer to Figure \ref{fig::SPStoLHCInjection}). These readings come in 10 second windows around the injection of a bunch in \ac{Gy/s}.

%BPMs
\paragraph{ }The \ac{BPM} were installed as a system for fast monitoring of the beam's position with respect to it's orbit drift \cite{Schmidt2006}. The trajectory offsets recorded by the \acs{BLM}s in the transfer lines must be minimised in order to reduce losses \cite{Drosdal2011}. In fact, if the change in orbit substantially exceeds its provided boundary values then the beam should be dumped \cite{Schmidt2006} so as to not cause any damage to the equipment. Unlike the \acs{TDI} \acs{BLM}s, the \acs{BPM} system is independent to the collimator system. For this study, the readings from the transfer line \acs{BPM}s around TI2 and TI8 will be used (refer to Figure \ref{fig::SPStoLHCInjection}). Raw values for these readings are stored by the \acs{LS} in \ac{mm} and are logged every 1 - 5 seconds on average.

%Abort Gap
\paragraph{ }When filling the \acs{LHC}, it is necessary to keep an abort gap of at least 3$\mu s$ in order to accommodate for the \ac{MKD} rise time \cite{Meddahi2010}. As the \ac{LHC} is filling to nominal intensity, this gap will be populated with un-trapped particles and particles leaking out of their \ac{RF} buckets \cite{Meddahi2010}. The \ac{AGM} was hence specifically designed to measure this particle population in the abort gap \cite{Lefevre2010}. This monitor can be found in Point 4 (refer to Figure \ref{fig::SPStoLHCInjection}) in the LHC \cite{Lefevre2010}. The raw values extracted for this study are stored in number of particles and come in 10 second groups around the moment of injection.
 
%SPS and LHC Intensities
\paragraph{ }The actual intensities of the circulating beam are measured by \ac{BCT}. For the \acs{LHC} in particular, a fast \acs{BCT} is used which is capable of monitoring a broad range of currents as it must be able to detect a single pilot bunch circulating the machine (of 10 $\mu$A) as well as the full nominal machine (over 0.5 mA) \cite{Jones2007}. These readings are then converted from amps to number of protons per beam and stored for analysis. The intensities for the \acs{LHC} come in 10 second groups around the moment of injection while the intensities for the \acs{SPS} give a single value of the intensity at the time of injection. 

%Number of Bunches
%TO DO


\section{Feature Scaling and Reduction Techniques}

%% Intro:
%% Curse of Dimensionality & Importance of Normalisation
\paragraph{ }Feature Scaling and Feature Reduction are two important pre-processing steps that should be considered when using machine learning in the data science process. Standard Scaling in particular will be used in this study as a pre-processing step to \ac{PCA}. Standard Scaling ensures that all the features have the properties of a standard normal distribution \cite{Scikitlearn}, which is especially important since \acs{PCA} involves finding the components that maximise the variance \cite{Shlens2014}. 

\paragraph{ }Apart from scaling, another challenge for outlier detection algorithms is data involving high dimensions since the contrast between different points diminishes as the number of dimensions increases \cite{Zimek2012}. This phenomenon is known as `The Curse of Dimensionality' and a technique to reduce the effect of this phenomenon is to use a dimension reduction technique and run the outlier detection algorithm on this new lower-dimensioned dataset. In this study, \acs{PCA} will be used as a dimension reduction technique.

%% PCA:
\paragraph{ }\acs{PCA} uses statistical and mathematical techniques to reduce the dimension of large data sets, thus allowing a large data set to be interpreted in less variables called principal components \cite{Richardson2009}. This technique works with the hope that the variance explained by an acceptably small number of principal components is large enough to explain the underlying structure of the dataset reasonably \cite{Shlens2014}. In fact, this non-parametric method has been used as a means of revealing the simplified structures' underlying complex datasets with minimal effort. The fact that this technique is non-parametric gives it the advantage that each result is unique and only dependent on the provided data set since no parameter tweaking is required \cite{Shlens2014}, however this is also a weakness of \acs{PCA} as there is no way of exploiting prior expert knowledge on the data set.

\section{Unsupervised Anomaly Detection Techniques}

\paragraph{ }Unsupervised machine learning algorithms refer to the class of machine learning algorithms where only the input features are available to the learner as there is no access to output labels corresponding to each input feature vector, or the aim of the algorithm is simply to observe or detect patterns in the available data. A. Hyv\"{a}rinen states in \cite{Hyvarinen2015} that some of the goals of unsupervised learning include data visualisation, noise reduction, feature extraction and finding interesting components; all of which are of particular interest in this study.

\paragraph{ }\ac{DBSCAN} and \ac{LOF} will both be used as unsupervised anomaly detection algorithms to detect and classify anomalous injections of the past year. Furthermore when working in 3 dimensions or less, these points can also be visualised to help the reader understand better the cause of these anomalies. 

\paragraph{ }\acs{DBSCAN} was created out of the necessity of having a clustering algorithm with the following requirements:
\begin{enumerate}
	\item ``Minimal requirements of domain knowledge to determine the input parameters,''
	\item ``Discovery of clusters with arbitrary shape,'' and
	\item ``Good efficiency on large databases'' \cite{Ester1996}
\end{enumerate}
\acs{DBSCAN} manages to attain these requirements by viewing clusters as ``areas of high density separated by areas of low density'' \cite{Sklearn2}. The points with a lower density will thus be considered as anomalies when compared to the regular clusters which have a higher density. This algorithm also introduces the concept of \textit{core samples} which was then used in the design of other unsupervised anomaly detection algorithms such as \acs{LOF}. 

\paragraph{ }The word \textit{factor} in \acs{LOF} refers to a ``degree of outlier-ness'' that this algorithm considers for each point in the data rather than using the concept that ``being an outlier is binary'' \cite{Breunig2000}. This algorithm uses a clustering technique which takes concepts from \acs{DBSCAN} to measure the \acs{LOF} of each point where a \acs{LOF} value greater than 1 implies that the point has a lower density than its neighbours and is thus probably an outlier.
%%To DO: MORE ON LOF

\section{Anomaly Detection at CERN}

\paragraph{ }In the paper released entitled ``Opportunities in Machine Learning for Particle Accelerators'' \cite{Edelen2018}, it was stated that due to the ''large number of process variables, non-linear behaviour, and many interacting subsystems,'' conventional analysis techniques on today's particle accelerator data is often insufficient and thus machine learning could be used as a means of anomaly detection. Furthermore, the authors also stated that these techniques could be used to detect ``subtle behaviours of key variables prior to negative events'' and they can also be used to ``identify and throw away bad signals.''

\paragraph{ }In his Master's Thesis, A. Halilovic used anomaly detection techniques solely on data obtained from the injection kicker magnets \cite{Halilovic2018}. Halilovic made use of a \ac{GMM} and Isolation Forests to detect anomalies however found that the best performance achieved by his proposed pipeline ``leaves something to be desired'' as too many anomalies were not correctly classified. The author also goes on to suggest that analysing \acs{LHC} data using the \acs{LOF} class provided in `\textit{scikit-learn}' could lead to interesting results.

\paragraph{ }Wielgosz, \textit{et. al.} also wrote a scientific paper on using anomaly detection techniques on the \acs{LHC} magnets \cite{Wielgosz2017}. This time, the authors went for a supervised approach and used Recurrent Neural Networks. They found that using adaptive quantisation to reduce 20-bit inputs into a 4-bit representation was an essential step in improving the algorithm's performance. The authors also stated that these anomaly detection techniques being proposed should not only be considered useful for \acs{CERN} equipment but also useful in the broader field of anomaly detection on time series data.

\paragraph{ }In 2017, Valentino \textit{et. al.} released a paper on using anomaly detection techniques ``to detect minor changes in the loss maps over time due to collimator settings errors or orbit variations'' \cite{Valentino2017}. The authors used \acs{PCA} as a dimension reduction technique and then applied \acs{LOF} on the resulting 2 dimensional data. Their proposed method was shown to positively identify these anomalous loss maps based solely on \acs{BPM} and \acs{BLM} readings. Furthermore, they proposed using this technique to monitor losses during fills of the \acs{LHC}.

\section{Software Implementation}
\paragraph{ }Although performance of k-means and k-Nearest Neighbours is not as optimal as in other Python packages such as `\textit{PyMVPA}' \cite{PyMVPA} or `\textit{shogun}' \cite{Shogun} (see Table 1 in \cite{Pedregosa2011}), it was decided to use the `\textit{scikit-learn}' machine learning package for this study due to its ``state-of-the-art implementation'' and ``easy-to-use interface tightly integrated with the Python language'' \cite{Pedregosa2011}. Furthermore, the algorithms implemented using this package can be ``used as building blocks for approaches specific to a use case'' \cite{Pedregosa2011} which will be useful if one would like to extend the scope of this study.